



From work_notes.txt:

EM for clustering?

Ideas from discussion 06/08/2018:
1. Overall workflow of descriptor program should be:
a. Take in certain biological function as input (say, break down substrate A)
b. Using pre-determined clusters (see master's thesis), automatically determine elementary functions required.
c. For each elementary function, retrieve corresponding descriptor, which contains structural parameter tolerance ranges, examples, possible dependence on structural parameters of other nodes, etc.
d. Using these ranges, supply to Rosetta for optimisation, to determine ideal configuration.

Descriptor therefore acts like a pre-optimiser, decreasing search space for Rosetta.

Rosetta might be able to do the whole process by brute force, through getting function matching and energy minimising. Descriptor therefore allows a narrowing of the search space, instead of everything possible, just search within these ranges with these tolerances/standard deviations. 

2. For master's thesis, 3 points:
a. Link to descriptor, issue with thesis using sequence only while descriptor need structural info matching. Structural info matching will allow for far more accurate clusters, but then is it possible? Many more dimensions, requiring a far larger dataset? Also, how do we "make" a similarity criterion/parameter? Possible?

b. Might be able to come up with better cluster algorithm. E.g. use EM, then can skip the whole minimising-number-of-cluster part.
But, Alex has apparently done some implementations, they also have manuscripts, wait for him to send you first to see how to make better. Alex is stuck on how to make more accurate (cluster?) predictions. Wait for him to send you stuff before starting work, igor has been informed.

c. thesis algorithm can be used as a fallback, in case insufficient dataset/structural info, then just use sequence to cluster, maybe. Or this can be reflected in relevant weight metric, increase weights for small datasets to sequence, and decrease for larger datasets, where other structural parameters will be better defined then. 

################################

From NOTES.txt, 13/7/2018:

Each node compare separately. Match node by node, no need scoring, we're creating a descr based on nodes from different loops.

-180 in phipsi correspond to 180 (torus). But this also brings in the issue of difference in "cost" between say changes in 100 => 120 of phipsi vs 80 => 100, or phi vs psi. Also, other discrete factors, e.g. b-b hbond less "far" than B-S hbonds

Discussion notes 0506:
For hbonds, we will eventually move on to a function, where instead of me having to match a node with a hbond, with another node with that hbond, we'll match based on possibility, in that if the old node has a hbond but the new one don't have one, can the old one survive without having one?

Also, general cleaning up and documentation of project. 

Energy for Rosetta:
http://scc.acad.bg/ncsa/articles/library/Library2016_Supercomputers-at-Work/Rosetta3.4_Protein-proteinIteraction/PyRosetta_Manual.pdf
(somewhere in middle)

Top 4 terms:
core.scoring: 
------------------------------------------------------------
 Scores                       Weight   Raw Score Wghtd.Score
------------------------------------------------------------
 fa_atr                       1.000   -4240.991   -4240.991
 fa_rep                       0.550    4336.451    2385.048
 fa_sol                       1.000    2798.424    2798.424
 fa_intra_rep                 0.005    3034.836      15.174
 fa_intra_sol_xover4          1.000     180.944     180.944
 lk_ball_wtd                  1.000     -56.460     -56.460
 fa_elec                      1.000    -933.702    -933.702
 pro_close                    1.250     270.565     338.206
 hbond_sr_bb                  1.000    -181.224    -181.224
 hbond_lr_bb                  1.000    -130.586    -130.586
 hbond_bb_sc                  1.000     -58.168     -58.168
 hbond_sc                     1.000     -38.850     -38.850
 dslf_fa13                    1.250       0.000       0.000
 omega                        0.400     149.145      59.658
 fa_dun                       0.700    4635.733    3245.013
 p_aa_pp                      0.600    -101.427     -60.856
 yhh_planarity                0.625       0.000       0.000
 ref                          1.000     188.455     188.455
 rama_prepro                  0.450     591.830     266.324
---------------------------------------------------
 Total weighted score:                     3776.412


Question to ask: 
For rosetta's scoring function, 


########################
From todo.txt, 13/7/2018:

Final program should have:
1. input pdb files for training
2. when given a new pdb file with designated sequence to test, it should:
	a. determine signature
	b. based on signature, retrieve relevant descriptor set
	c. determine conditional probability value of that
	d. multiply this probability value with the signature probability value

For that, program need to have 1. signature, and 2. probability distributions 
based on residue and position. 

we'll therefore calculate everything first, and then split based on residue for each position.

Need a separate script from plot.py to do that. 

So, index by relative_sno, and then index by res. Then output probability distribution (as what form though? we already have all the points.) 

For now, we'll assume that apart from signature residue, the rest are independent.

So, i need a query function, how well does this new datapoint cluster with the existing set of datapoints. 

First, plot it out in 2D to see, then assume normal distribution for each? Plot to verify that it is normal.

Task: Focus on building the plot first.

########################

# 08/07/2019

TODO:

1. From Ioncom, cut out the seqs centering on the matched points
2. Check converge to see if it exclude areas of seq that match to previous parts. If it does, turn off that bit.
3. Remove duplicates from previous cut areas
4. Run converge...?
5. Check overlaps. Prioritise those with better matches, maybe. 
